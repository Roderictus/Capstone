---
title: "Milestone Report"
author: "Rodrigo Franco Fuentes"
date: "27/10/2018"
output:
  ioslides_presentation: default
  slidy_presentation: default
---




```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
#Packages
#library(dplyr)
#library(tidyr)
#library(tidytext)
library(tidyverse)
#library(ggplot2)
library(igraph)
library(tidytext)
```
## I.  Basic report of summary statistics using the data sets
There are three sources of data: Blog, News and Tweet. Each has an entry corresponding different formats, Blog being the longest, Tweet the shortest. In terms of number of words the three texts are similar. As part of the document analysis data has been partially tidied, removing stop words, although these may become handy later. Removing stop words reduces the data set total size to less than Half, blog has the highest account of stop words, News has sustantially less.

```{r , echo = FALSE, cache = TRUE}
T1 <- read.table(file = "Table1.csv")
T1
```

## II.Report Findings
The following table shows the ocurrences of specific values for each 100,000 tidy words. From a glance tweet and blog seem to be more sentiment driven with a higher frequency of words such as love, happy, life. Retweet or "rt", is the third most common "word", which signals a need for further cleaning of the data assuming it would bring incorrect predictions outside of a tweeter prediction app context.  

##Top Words by Source and Frequency (for each 100,000)
```{r Report Findings, echo =  FALSE, cache = TRUE,}
T2 <- read.csv(file = "Table2.csv")
options(digits = 2)
T2[,-c(1)]
```

## Common bigrams 
Further procesing will require n-grams, here we can see the most common bigrams in tidy Blog data. We may need to go back on the tidying since many of the words we want to predict may be stop words. But also we may find that the most common bigrams are pairs of stop words as our bigram frequency table indicates. Here we may find that bigrams repeat stop words in a continous nonsensical way (ej. "I have of the i was...") This may be solved with higher order n-grams or by restricting stop words recurrence. 





## Going further, plans on prediction app

The main method of predicting text will be, in principle, a Katz back off model, attempting further refinements with latent Dirichlet allocation for cluster subsetting. A sentiment subseting may also be tried.


Tidy data should be valuable for establishing the tone and thematical clustering techniques we hope will help increase the accuracy of the text prediction app. 


---
title: "Milestone Report"
author: "Rodrigo Franco Fuentes"
date: "27/10/2018"
output:
  ioslides_presentation: default
  slidy_presentation: default
---
```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
#Packages
#library(dplyr)
#library(tidyr)
#library(tidytext)
library(tidyverse)
#library(ggplot2)
<<<<<<< HEAD
#library(igraph)
#library(ggraph)
#library(tidytext)


##Reading files
#Blog_Eng<-tbl_df(read_lines(file = "./final/en_US/en_US.blogs.txt"))
#News_Eng<-tbl_df(read_lines(file = "./final/en_US/en_US.news.txt"))
#Tweet_Eng<-tbl_df(read_lines(file = "./final/en_US/en_US.twitter.txt"))

###Tokenizing into sentences and words
#Temporally Subsetting to reduce time
#Transforming from character to data frame 

#Tokenizing
Token_Blog <- Blog_Eng %>%
      mutate(Line = row_number()) %>%
      unnest_tokens(output = word, input = value)

#Token_News <- News_Eng %>%
#      mutate(Line = row_number()) %>%
#      unnest_tokens(output = word, input = value)

#Token_Tweet <- Tweet_Eng %>%
#      mutate(Line = row_number()) %>%
#      unnest_tokens(output = word, input = value)

#removing stop words
#data(stop_words)
#tidy_Blog <- Token_Blog %>%
#      mutate(source = "English Blog") %>%
#      anti_join(stop_words)

#tidy_News <- Token_News %>%
#      mutate(source = "English News") %>%
#      anti_join(stop_words)

#tidy_Tweet <- Token_Tweet %>%
#      mutate(source = "English News") %>%
#      anti_join(stop_words)

#joining data sets
#Tidy_All<- bind_rows(tidy_Blog, tidy_News)
#Tidy_All <- bind_rows(Tidy_All,tidy_Tweet)
#write.table(Tidy_All, "./Tidy_All_English.txt", sep = "\t")
#gc()
Tidy_all<-read.table("Tidy_All_English.txt")

data(stop_words)
stop_words

#Word distribution


####Most common non stop-words all DataSets
Tidy_all %>%
      count(word, sort = TRUE) %>%
      filter(n >40000) %>%
      mutate(word = reorder(word, n )) %>%
      ggplot(aes(word,n)) +
      geom_col() + 
      coord_flip()

Tidy_all %>%
      group_by(source) %>%
      count( word, sort = TRUE)

gc()
#characteristic words for the texts inverse word frequency
      
=======
library(igraph)
library(tidytext)
>>>>>>> 514d8cb4eec9c8fec68ea7b6aa62d635fd888885
```
## I.  Basic report of summary statistics using the data sets
There are three sources of data: Blog, News and Tweet. Each has an entry corresponding different formats, Blog being the longest, Tweet the shortest. In terms of number of words the three texts are similar. As part of the document analysis data has been partially tidied, removing stop words, although these may become handy later. Removing stop words reduces the data set total size to less than Half, blog has the highest account of stop words, News has substantially less.

```{r , echo = FALSE, cache = TRUE}
T1 <- read.table(file = "Table1.csv")
T1
```

## II. Report Findings
The following table shows the occurrences of specific values for each 100,000 tidy words. From a glance tweet and blog seem to be more sentiment driven with a higher frequency of words such as love, happy, life. Retweet or "rt", is the third most common "word", which signals a need for further cleaning of the data assuming it would bring incorrect predictions outside of a tweeter prediction app context.  

<<<<<<< HEAD
=======
##Top Words by Source and Frequency (for each 100,000)
```{r Report Findings, echo =  FALSE, cache = TRUE,}
T2 <- read.csv(file = "Table2.csv")
options(digits = 2)
T2[,-c(1, 4:6)]
```

## III. Common bigrams 
Further processing will require n-grams, here we can see the most common bigrams in tidy Blog data. We may need to go back on the tidying since many of the words we want to predict may be stop words. But also we may find that the most common bigrams are pairs of stop words as our bigram frequency table indicates. Here we may find that bigrams repeat stop words in a continuous nonsensical way (ej. "I have of the i was...") This may be solved with higher order n-grams or by restricting stop words recurrence. 

## Most Common Bigrams
```{r Report , echo =  FALSE, cache = TRUE}
CBgraph2 <- read.csv("CBgraph.csv")
CBgraph2 <- CBgraph2[,-c(1,2)]
G1 <- CBgraph2 %>% 
  mutate(bigram = reorder(bigram, n)) %>% 
  ggplot(aes(bigram,n)) +
  geom_col(show.legend = FALSE, fill = "orange") + 
  labs(y = "Bigram", x = NULL) + 
  ggtitle("Bigram Ocurrence in Blog Data Set, With Stop Words") +
  coord_flip()
G1
```

## IV. Going further, plans on prediction app
>>>>>>> 514d8cb4eec9c8fec68ea7b6aa62d635fd888885

The main method of predicting text will be, in principle, a Katz back off model, attempting further refinements with latent Dirichlet allocation for cluster and sentiment subseting may also be tried. Next step is investigating about efficient implementations of this algorithm, dynamic subsetting, and further tidying of the data. 




